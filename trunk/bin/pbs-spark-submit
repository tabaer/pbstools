#!/usr/bin/python
#
# pbs-spark-submit:  Run an Apache Spark "job" (including optionally
#                    starting the Spark services) inside a PBS job.
# Copyright 2014, 2015 University of Tennessee
#
# License:  GNU GPL v2; see ../COPYING for details.
# Revision info:
# $HeadURL$
# $Revision$
# $Date$
import getopt
import glob
import os
import platform
import sys
import time

#
# ways to launch workers
#
class Launcher:
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],tpn=None):
        raise NotImplementedError
    def sleep(self):
        sleeptime = 5
        if ( "PBS_NUM_NODES" in os.environ.keys() ):
            sleeptime += 2*int(os.environ["PBS_NUM_NODES"])
        time.sleep(sleeptime)
    def env_list(env,prop_env_list):
        # since we can't rely on ssh_config and sshd_config having
        # the appropriate SendEnv/AcceptEnv settings
        argv = []
        for var in prop_env_list:
            if ( var in env.keys() ):
                argv.append(var+"="+env[var])
        return argv
    def env_string(env,prop_env_list):
        return " ".join(self.env_list(env,prop_env_list))


class ExecLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],tpn=None):
        time.sleep(1)
        
        # lots of squick to try to limit the number of cores used on big
        # SMP/NUMA systems that are likely shared with other users
        cpuset = None
        cpusetroot = None
        cpus = 0
        if ( os.path.exists("/proc/self/cpuset") ):
            cpusetfile = open("/proc/self/cpuset")
            cpuset = cpusetfile.read().rstrip("\n")
            cpusetfile.close()
        if ( os.path.exists("/dev/cpuset") ):
            cpusetroot = "/dev/cpuset"
        elif ( os.path.exists("/sys/fs/cgroup/cpuset") ):
            cpusetroot = "/sys/fs/cgroup/cpuset"
        if ( cpusetroot is not None and cpuset is not None ):
            cpusfile = None
            if ( os.path.exists(cpusetroot+cpuset+"/cpus") ):
                cpusfile = open(cpusetroot+cpuset+"/cpus")
            elif ( os.path.exists(cpusetroot+cpuset+"/cpuset.cpus") ):
                cpusfile = open(cpusetroot+cpuset+"/cpuset.cpus")
            if ( cpusfile is not None ):
                allcpus = cpusfile.read()
                cpusfile.close()
                for cgroup in allcpus.split(","):
                    cpurange = cgroup.split("-")
                    if ( len(cpurange)==1 ):
                        cpus += 1
                    elif ( len(cpurange)==2 ):
                        cpus += int(cpurange[1])-int(cpurange[0])+1
        if ( cpus==0 and "PBS_NP" in os.environ.keys() ):
            try:
                cpus = int(os.environ["PBS_NP"])
            except e,Exception:
                pass
        if ( cpus>0 ):
            os.environ["SPARK_WORKER_CORES"] = str(cpus)
            env["SPARK_WORKER_CORES"] = str(cpus)
        # need to do the equivalent shenanigans for memory at some point...

        # base functionality
        argv = cmdline.split()
        if ( propagate_env ):
            for arg in self.env_list(env,prop_arg_list):
                argv.append(arg)
        child_pid = os.fork()
        if ( child_pid==0 ):
            os.execvpe(argv[0],argv,env)
        self.sleep()


class PBSDSHLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],tpn=None):
        time.sleep(1)
        cmd = cmdline
        if ( propagate_env ):
            cmd = self.env_string(env,prop_env_list)+" "+cmdline
        if ( tpn is None ):
            os.system("pbsdsh "+cmdline+" &")
        else:
            nodes = nodelist(unique=True)
            for node in nodes:
                for i in range(int(tpn)):
                    os.system("pbsdsh -h "+node+" "+cmdline+" &")
        self.sleep()


class SSHLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],tpn=None):
        time.sleep(1)
        if ( "PBS_NODEFILE" in os.environ.keys() ):
            if ( tpn is None ):
                nodes = nodelist()
            else:
                nodes = nodelist(unique=True)
            for node in nodes:
                argv = cmdline.split()
                ssh = "ssh"
                if ( "SPARK_SSH" in env.keys() ):
                    ssh=env["SPARK_SSH"]
                argv.insert(0,ssh)
                argv.insert(1,node)
                if ( propagate_env ):
                    for arg in self.env_list(env,prop_env_list):
                        argv.insert(2,arg)
                sys.stderr.write(" ".join(argv)+"\n")
                if ( tpn is None ):
                    nforks = 1
                else:
                    nforks = int(tpn)
                for i in range(nforks):
                    child_pid = os.fork()
                    if ( child_pid==0 ):
                        os.execvpe(argv[0],argv,env)   
            self.sleep()
        else:
            raise EnvironmentError("PBS_NODEFILE undefined")

#
# functions to help with PBS node file
#
def nodelist(unique=False):
    nodes = []
    if ( "PBS_NODEFILE" in os.environ.keys() ):
        nodefile = open(os.environ["PBS_NODEFILE"])
        for line in nodefile.readlines():
            node = line.rstrip("\n")
            if ( not unique or not ( node in nodes ) ):
                nodes.append(node)
    return nodes


#
# functions to help with handling Java properties
#
def propsToCmdLine(proplist):
    result = []
    for prop in proplist.keys():
        result.append("-D"+prop+"=\""+proplist[prop]+"\"")
    return " ".join(result)

def propsFromFile(filename):
    if ( not os.path.exists(filename) ):
        raise IOError(filename+" not found")
    proplist = {}
    fd = open(filename)
    for line in fd.readlines():
        if ( not line.startswith("#") ):
            keyval = (line.rstrip("\n")).split("=",1)
            if ( len(keyval)==2 ):
                proplist[keyval[0]] = keyval[1]
    return proplist

#
# documentation
#
def usage():
    sys.stderr.write("pbs-spark-submit:  Run an Apache Spark \"job\" (including optionally\n\tstarting the Spark master and work services) inside a PBS job.\n")
    sys.stderr.write("\n")
    sys.stderr.write("Usage:  pbs-spark-submit [arguments] <app.jar|python_file> [app options]\n")
    sys.stderr.write("\n")
    sys.stderr.write("Options:\n")
    sys.stderr.write("\t--help or -h\n\t\tPrint this help message\n")
    sys.stderr.write("\t--init\n\t\tInitialize Spark master/worker services (default).\n")
    sys.stderr.write("\t--no-init\n\t\tDo not initialize Spark master/worker services.\n")
    sys.stderr.write("\t--exec\n\t\tUse the exec process launcher.\n")
    sys.stderr.write("\t--pbsdsh\n\t\tUse the pbsdsh process launcher (default).\n")
    sys.stderr.write("\t--ssh\n\t\tUse the ssh process launcher.\n")
    sys.stderr.write("\t--conf-dir <confdir> or -C <confdir>\n\t\tLook in <confdir> for Java properties files.\n")
    sys.stderr.write("\t--log-dir <logdir> or -L <logdir>\n\t\tPlace logs in <logdir>.\n")
    sys.stderr.write("\t--log4j-properties <propsfile> or -l <propsfile>\n\t\tRead log4j properties from <propsfile>.\n")
    sys.stderr.write("\t--work-dir <workdir> or -d <workdir>\n\t\tUse <workdir> as Spark program's working directory.\n")
    sys.stderr.write("\t--memory <memlimit> or -m <memlimit>\n\t\tSet per-worker memory limit.\n")
    sys.stderr.write("\t--pausetime <N> or -p <N>\n\t\tPause <N> seconds between startup stages (default 5).\n")
    sys.stderr.write("\t-D <key>=<value>\n\t\tSet the Java property <key> to <value>.\n")
    sys.stderr.write("\t--properties-file <propfile> or -P <propfile>\n\t\tRead Java properties from <propfile>.\n")
    sys.stderr.write("\n")
    sys.stderr.write("Run \"man pbs-spark-submit\" for more details.\n")
    sys.stderr.write("\n")
    sys.exit(0)
    

#
# main program begins here
#

# sanity checks
if ( not ( "PBS_JOBID" in os.environ.keys() ) ):
    raise EnvironmentError("Not in a PBS job")
if ( not ( "SPARK_HOME" in os.environ.keys() ) ):
    raise EnvironmentError("SPARK_HOME not defined")

# set up default environment
init_svcs = True
memlimit = None
child_args = []
properties = {}
pausetime = 5
launcher = PBSDSHLauncher()
tpn = None
log4j_props = None
if ( "SPARK_LAUNCHER" in os.environ.keys() ):
    if ( os.environ["SPARK_LAUNCHER"] in ("exec","EXEC") ):
        launcher = ExecLauncher()
    if ( os.environ["SPARK_LAUNCHER"] in ("pbsdsh","PBSDSH") ):
        launcher = PBSDSHLauncher()
    if ( os.environ["SPARK_LAUNCHER"] in ("ssh","SSH") ):
        launcher = SSHLauncher()
if ( not ( "SPARK_CONF_DIR" in os.environ.keys() ) ):
    os.environ["SPARK_CONF_DIR"] = os.getcwd()+"/conf"
if ( not ( "SPARK_LOG_DIR" in os.environ.keys() ) ):
    os.environ["SPARK_LOG_DIR"] = os.getcwd()

# manage scratch directories
# **ASSUMPTION**:  work directory is on a shared file system
workdir = os.getcwd()
if ( "SCRATCHDIR" in os.environ.keys() ):
    workdir = os.environ["SCRATCHDIR"]+"/spark-"+os.environ["PBS_JOBID"]
# SPARK_LOCAL_DIRS should be node-local
if ( ( "TMPDIR" in os.environ.keys() ) and
     not ( "SPARK_LOCAL_DIRS" in os.environ.keys() ) ):
    os.environ["SPARK_LOCAL_DIRS"] = os.environ["TMPDIR"]
elif ( not ( "SPARK_LOCAL_DIRS" in os.environ.keys() ) ):
    os.environ["SPARK_LOCAL_DIRS"] = "/tmp"

# command line argument handling
try:
    opts, child_args = getopt.getopt(sys.argv[1:],"C:D:d:hL:l:m:P:p:t:",["help","init","no-init","exec","pbsdsh","ssh","conf-dir","log-dir=","log4j-properties=","work-dir=","memory=","properties-file","pause-time","tpn=","tasks-per-node="])
except getopt.GetoptError, err:
    sys.stderr.write(str(err)+"\n")
    usage()
for opt in opts:
    if ( opt[0]=="--no-init" ):
        init_svcs = False
    elif ( opt[0]=="--init" ):
        init_svcs = True
    elif ( opt[0]=="--help" or opt[0]=="-h" ):
        usage()
    elif ( opt[0]=="--exec" ):
        launcher = ExecLauncher()
    elif ( opt[0]=="--pbsdsh" ):
        launcher = PBSDSHLauncher()
    elif ( opt[0]=="--ssh" ):
        launcher = SSHLauncher()
    elif ( opt[0]=="--conf-dir" or opt[0]=="-C" ):
        os.environ["SPARK_CONF_DIR"] = opt[1]
    elif ( opt[0]=="--log-dir" or opt[0]=="-L" ):
        os.environ["SPARK_LOG_DIR"] = opt[1]
    elif ( opt[0]=="--log4j-properties" or opt[0]=="-l" ):
        log4j_props = opt[1]
    elif ( opt[0]=="--work-dir" or opt[0]=="-d" ):
        workdir = opt[1]
    elif ( opt[0]=="--memory" or opt[0]=="-m" ):
        memlimit = opt[1]
    elif ( opt[0]=="-D" ):
        keyval = opt[1].split("=",1)
        if ( len(keyval)==2 ):
            properties[keyval[0]] = keyval[1]
        else:
            raise getopt.GetoptError("malformed property \""+opt[1]+"\"")
    elif ( opt[0]=="--properties-file" or opt[0]=="-P" ):
        if ( os.path.exists(opt[1]) ):
            props = propsFromFile(opt[1])
            for key in props.keys():
                properties[key] = props[key]
    elif ( opt[0]=="--pause-time" or opt[0]=="-p" ):
        pausetime = opt[1]
    elif ( opt[0]=="--tpn" or opt[0]=="--tasks-per-node" or
           opt[0]=="-t" ):
        tpn = opt[1]

# read any properties files in the conf directory
for propfile in glob.glob(os.environ["SPARK_CONF_DIR"]+"/*.properties"):
    if ( os.path.exists(propfile) ):
        props = propsFromFile(propfile)
        for key in props.keys():
            if ( not ( key in properties.keys() ) ):
                properties[key] = props[key]

# make sure the work dir actually exists
if ( workdir is not None and not os.path.exists(workdir) ):
    os.mkdir(workdir)

# if certain props aren't set, set some sane defaults,
# inspired by http://mbonaci.github.io/mbo-spark/
# This is mostly to prevent Spark defaulting to stupid things
# like trying to write logs in $SPARK_HOME/logs without
# checking if it's writeable...
# Currently commented out because it's not working
#if ( not ( "log4j.rootCategory" in properties.keys() ) ):
#    properties["log4j.rootCategory"] = "ERROR,console"
#if ( not ( "log4j.appender.console" in properties.keys() ) ):
#    properties["log4j.appender.console"] = "org.apache.log4j.ConsoleAppender"
#if ( not ( "log4j.appender.console.target" in properties.keys() ) ):
#    properties["log4j.appender.console.target"] = "System.err"
#if ( not ( "log4j.appender.console.layout" in properties.keys() ) ):
#    properties["log4j.appender.console.layout"] = "org.apache.log4j.PatternLayout"
#if ( not ( "log4j.appender.console.layout.ConversionPattern" in properties.keys() ) ):
#    properties["log4j.appender.console.layout.ConversionPattern"] = "%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n"

# **ASSUMPTION**:  master runs on mother superior node
os.environ["SPARK_MASTER_IP"] = platform.node()
if ( not ( "SPARK_MASTER_PORT" in os.environ.keys() ) ):
    os.environ["SPARK_MASTER_PORT"] = "7077"
os.environ["SPARK_MASTER"] = "spark://"+os.environ["SPARK_MASTER_IP"]+":"+str(os.environ["SPARK_MASTER_PORT"])
#sys.stderr.write("Spark master = "+os.environ["SPARK_MASTER"]+"\n")

if ( init_svcs ):
    # stick any properties in the appropriate environment variable
    if ( len(properties)>0 ):
        if ( "SPARK_DAEMON_JAVA_OPTS" in os.environ.keys() ):
            os.environ["SPARK_DAEMON_JAVA_OPTS"] += " "+propsToCmdLine(properties)
        else:
            os.environ["SPARK_DAEMON_JAVA_OPTS"] = propsToCmdLine(properties)

    # launch master on mother superior
    cmdline = os.environ["SPARK_HOME"]+"/sbin/start-master.sh"
    os.system(cmdline+" &")
    sys.stderr.write(cmdline+"\n")
    sys.stdout.write("SPARK_MASTER="+os.environ["SPARK_MASTER"]+"\n")
    time.sleep(pausetime)

    # launch workers
    cmdline = os.environ["SPARK_HOME"]+"/bin/spark-class org.apache.spark.deploy.worker.Worker"
    if ( memlimit is not None ):
        cmdline += " --memory "+memlimit
    if ( workdir is not None ):
        cmdline += " --work-dir "+workdir
    cmdline += " "+os.environ["SPARK_MASTER"]
    sys.stderr.write(cmdline+"\n")
    launcher.launch(cmdline,os.environ,tpn=tpn)
    time.sleep(pausetime)

# run the user's Spark "job", if one is given
if ( len(child_args)>0 ):
    cmdline = os.environ["SPARK_HOME"]+"/bin/spark-submit --master "+os.environ["SPARK_MASTER"]
    if ( log4j_props is not None ):
        cmdline += " --driver-java-options -Dlog4j.configuration=file:"+log4j_props
    cmdline += " "+" ".join(child_args)
    os.system(cmdline)
